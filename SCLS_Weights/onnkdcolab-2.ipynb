{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onnkdcolab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P6xVY6OJWIJ"
      },
      "outputs": [],
      "source": [
        "# pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "def dataload(key, bs):\n",
        "    '''data agumentaiton'''\n",
        "\n",
        "    if key == 'HIGH10':\n",
        "        traindir ='../data/train/'\n",
        "        testdir = '../data/test/'\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                             std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        train_dataset = datasets.ImageFolder(\n",
        "            traindir,\n",
        "            transforms.Compose([\n",
        "                transforms.Resize((256,256)),\n",
        "                transforms.RandomResizedCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]))\n",
        "\n",
        "        test_dataset = datasets.ImageFolder(\n",
        "            testdir,\n",
        "            transforms.Compose([\n",
        "                transforms.Resize((224,224)),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]))\n",
        "    if key == 'MNIST':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        train_dataset = datasets.MNIST(root='MNIST', train=True, download=True,\n",
        "                                       transform=transform)\n",
        "        test_dataset = datasets.MNIST(root='MNIST', train=False,\n",
        "                                      transform=transform)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset,batch_size=bs, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(test_dataset,batch_size=bs, shuffle=True)\n",
        "\n",
        "    return trainloader, testloader\n"
      ],
      "metadata": {
        "id": "PVdNeOk4Jdr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from scipy import signal\n",
        "import scipy\n",
        "from torch import nn\n",
        "import time\n",
        "\n",
        "\n",
        "def dft_conv(imgR, imgIm, kernelR, kernelIm):\n",
        "    # Fast complex multiplication\n",
        "    #print(kernelR.shape, imgR.shape)\n",
        "    ac = torch.mul(kernelR, imgR)\n",
        "    bd = torch.mul(kernelIm, imgIm)\n",
        "\n",
        "    ab_cd = torch.mul(torch.add(kernelR, kernelIm), torch.add(imgR, imgIm))\n",
        "    # print(ab_cd.sum(1)[0,0,:,:])\n",
        "    imgsR = ac - bd\n",
        "    imgsIm = ab_cd - ac - bd\n",
        "\n",
        "    # Sum over in channels\n",
        "    imgsR = imgsR.sum(1)\n",
        "    imgsIm = imgsIm.sum(1)\n",
        "\n",
        "    return imgsR, imgsIm\n",
        "\n",
        "\n",
        "class FFT_Conv_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, imgSize, inCs, outCs1, outCs2, outCs3, imagDim, filtSize, cuda=False):\n",
        "        super(FFT_Conv_Layer, self).__init__()\n",
        "        self.filts1 = np.random.normal(0, 0.01, (1, inCs, outCs1, filtSize, filtSize, imagDim))\n",
        "        self.filts2 = np.random.normal(0, 0.01, (1, outCs1, outCs2, filtSize, filtSize, imagDim))\n",
        "        self.filts3 = np.random.normal(0, 0.01, (1, outCs2, outCs3, filtSize, filtSize, imagDim))\n",
        "        self.imgSize = imgSize\n",
        "        self.filtSize = np.size(self.filts1, 4)\n",
        "\n",
        "        if cuda:\n",
        "            self.filts1 = torch.from_numpy(self.filts1).type(torch.float32).cuda()\n",
        "            self.filts1 = Parameter(self.filts1)\n",
        "\n",
        "            self.filts2 = torch.from_numpy(self.filts2).type(torch.float32).cuda()\n",
        "            self.filts2 = Parameter(self.filts2)\n",
        "\n",
        "            self.filts3 = torch.from_numpy(self.filts3).type(torch.float32).cuda()\n",
        "            self.filts3 = Parameter(self.filts3)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        # Pad and transform the image\n",
        "        # Pad arg = (last dim pad left side, last dim pad right side, 2nd last dim left side, etc..)\n",
        "\n",
        "        imgs = imgs.unsqueeze(2)\n",
        "        imgs = imgs.unsqueeze(5)\n",
        "\n",
        "        imgs = F.pad(imgs, (0, 0, 0, self.filtSize - 1, 0, self.filtSize - 1))\n",
        "        imgs = imgs.squeeze(5)\n",
        "\n",
        "        imgs = torch.rfft(imgs, 2, onesided=False)\n",
        "        # print(imgs.shape)\n",
        "\n",
        "        # Extract the real and imaginary parts\n",
        "        imgsR = imgs[:, :, :, :, :, 0]\n",
        "        imgsIm = imgs[:, :, :, :, :, 1]\n",
        "\n",
        "        # Pad and transform the filters\n",
        "        filts1 = F.pad(self.filts1, (0, 0, 0, self.imgSize - 1, 0, self.imgSize - 1))\n",
        "        filts2 = F.pad(self.filts2, (0, 0, 0, self.imgSize - 1, 0, self.imgSize - 1))\n",
        "        filts3 = F.pad(self.filts3, (0, 0, 0, self.imgSize - 1, 0, self.imgSize - 1))\n",
        "\n",
        "        filts1 = torch.fft(filts1, 2)\n",
        "        filts2 = torch.fft(filts2, 2)\n",
        "        filts3 = torch.fft(filts3, 2)\n",
        "\n",
        "        # Extract the real and imaginary parts\n",
        "        filt1R = filts1[:, :, :, :, :, 0]\n",
        "        filt1Im = filts1[:, :, :, :, :, 1]\n",
        "\n",
        "        filt2R = filts2[:, :, :, :, :, 0]\n",
        "        filt2Im = filts2[:, :, :, :, :, 1]\n",
        "\n",
        "        filt3R = filts3[:, :, :, :, :, 0]\n",
        "        filt3Im = filts3[:, :, :, :, :, 1]\n",
        "\n",
        "        # Do element wise complex multiplication\n",
        "        imgsR_old, imgsIm_old =imgsR, imgsIm \n",
        "        imgsR, imgsIm = dft_conv(imgsR, imgsIm, filt1R, filt1Im)\n",
        "        imgsR = imgsR.unsqueeze(2)\n",
        "        imgsIm = imgsIm.unsqueeze(2)\n",
        "        imgsR, imgsIm = dft_conv(imgsR, imgsIm, filt2R, filt2Im)\n",
        "        imgsR = imgsR.unsqueeze(2)\n",
        "        imgsIm = imgsIm.unsqueeze(2)\n",
        "        imgsR, imgsIm = dft_conv(imgsR, imgsIm, filt3R, filt3Im)\n",
        "        # print('ref',imgsR.shape)\n",
        "\n",
        "\n",
        "        # print(filt1R.shape, filt1Im.shape,filt2R.shape, filt2Im.shape)\n",
        "\n",
        "        f12r, f12i = dft_conv(filt1R.view(1,32,1,30,30), filt1Im.view(1,32,1,30,30),filt2R, filt2Im)\n",
        "        # print(f12r.shape)\n",
        "        f12r = f12r.unsqueeze(2)\n",
        "        f12i = f12i.unsqueeze(2)\n",
        "        f123r, f123i = dft_conv(f12r.view(1,128,1,30,30), f12i.view(1,128,1,30,30),filt3R, filt3Im)\n",
        "        f123r = f123r.unsqueeze(2)\n",
        "        f123i = f123i.unsqueeze(2)\n",
        "       \n",
        "        # print(f123r.shape)\n",
        "        imgsRnew, imgsImnew = dft_conv(imgsR_old, imgsIm_old,f123r.view(1,1,256,30,30),f123i.view(1,1,256,30,30))\n",
        "\n",
        "        imgsR, imgsIm = imgsRnew, imgsImnew \n",
        "        f123r = f123r.cpu().detach().numpy()\n",
        "        f123i = f123i.cpu().detach().numpy()\n",
        "        torch.save(f123r,'f123r.npy')\n",
        "        torch.save(f123i,'f123i.npy')\n",
        "        # print(imgsR.shape)\n",
        "\n",
        "        # assert(imgsR==imgsRnew)\n",
        "        # assert(imgsImnew==imgsIm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Add dim to concat over\n",
        "        imgsR = imgsR.unsqueeze(4)\n",
        "        imgsIm = imgsIm.unsqueeze(4)\n",
        "\n",
        "        # Concat the real and imaginary again then IFFT\n",
        "        imgs = torch.cat((imgsR, imgsIm), -1)\n",
        "        # print(\"1\",imgs.shape)\n",
        "        imgs = torch.ifft(imgs, 2)\n",
        "        # print(\"2\",imgs.shape)\n",
        "\n",
        "        # Filter and imgs were real so imag should be ~0\n",
        "        imgs = imgs[:, :, 1:-1, 1:-1, 0]\n",
        "        # print(\"3\",imgs.shape)\n",
        "        return imgs\n",
        "\n",
        "\n",
        "class StudentNetwork_noRelu(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(StudentNetwork_noRelu, self).__init__()\n",
        "        self.conv1 = FFT_Conv_Layer(imgSize=28, inCs=in_channels, outCs1=32, outCs2=128, outCs3=256, imagDim=2, filtSize=3, cuda=True)\n",
        "\n",
        "        self.conv2_bn = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(9216, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.dropout_input = 0.5\n",
        "        self.dropout_hidden = 0.5\n",
        "        self.is_training = True\n",
        "        self.avepool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        forw = self.conv1(x)\n",
        "\n",
        "\n",
        "        forw = self.conv2_bn(forw)\n",
        "        #forw = self.maxpool(forw)\n",
        "        forw = self.avepool(forw)\n",
        "        forw = forw.view(-1, 9216)\n",
        "        forw = F.dropout(forw, p=self.dropout_input, training=self.is_training)\n",
        "        forw = F.dropout(self.fc1(forw), p=self.dropout_hidden, training=self.is_training)\n",
        "        forw = F.relu(forw)\n",
        "        forw = self.fc2(forw)\n",
        "        forw = F.relu(forw)\n",
        "        forw = self.fc3(forw)\n",
        "        return self.m(forw)\n",
        "\n",
        "\n",
        "class Teacher_Network(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Teacher_Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
        "\n",
        "        self.conv2_bn = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(9216, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "        self.dropout_input = 0.5\n",
        "        self.dropout_hidden = 0.5\n",
        "        self.is_training = True\n",
        "        self.avepool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        forw = nn.functional.relu(self.conv1(x))\n",
        "        forw = nn.functional.relu(self.conv2(forw))\n",
        "        forw = nn.functional.relu(self.conv3(forw))\n",
        "\n",
        "        forw = self.conv2_bn(forw)\n",
        "        forw = self.avepool(forw)\n",
        "        forw = forw.view(-1, 9216)\n",
        "        forw = F.dropout(forw, p=self.dropout_input, training=self.is_training)\n",
        "        forw = F.dropout(self.fc1(forw), p=self.dropout_hidden, training=self.is_training)\n",
        "        forw = F.relu(forw)\n",
        "        forw = self.fc2(forw)\n",
        "        forw = F.relu(forw)\n",
        "        forw = self.fc3(forw)\n",
        "        return self.m(forw)"
      ],
      "metadata": {
        "id": "IKp8Hkr2Jx8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "\n",
        "\n",
        "class teacher_solver():\n",
        "\n",
        "    def __init__(self, train_loader, test_loader, model, criterion, student_optimizer,\n",
        "                 student_lr_scheduler,\n",
        "                 epochs, model_path, model_name):\n",
        "\n",
        "        self.model_path = model_path\n",
        "        self.model_name = model_name\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.model = model\n",
        "        self.student_optimizer = student_optimizer\n",
        "        self.student_lr_scheduler = student_lr_scheduler\n",
        "        self.epochs = epochs\n",
        "        self.criterion = criterion\n",
        "        self.step = 0\n",
        "\n",
        "    def train(self):\n",
        "        val_loss = None\n",
        "        print('epochs', self.epochs)\n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"Start Training...\")\n",
        "            self.val_predictions = []\n",
        "            self.val_gts = []\n",
        "            start = datetime.now()\n",
        "            tr_stu_avg_loss = self.train_loop()\n",
        "            val_stu_avg_loss, testaccuracy = self.validate()\n",
        "            print('-' * 50)\n",
        "            print('Summary: Epoch {0} | Time {1}s'.format(epoch, datetime.now() - start))\n",
        "            print('Train | Loss {0:.4f}'.format(tr_stu_avg_loss))\n",
        "            print('Validate | Loss {0:.4f}'.format(val_stu_avg_loss))\n",
        "            print('Validate | Accuracy {0:.4f}'.format(testaccuracy))\n",
        "            # load the model\n",
        "            if val_loss is None or val_stu_avg_loss < val_loss:\n",
        "                val_loss = val_stu_avg_loss\n",
        "                torch.save(self.model.state_dict(), self.model_path + self.model_name)\n",
        "                best_model = epoch\n",
        "            print('best_model is on epoch:', best_model)\n",
        "\n",
        "\n",
        "    def train_loop(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0\n",
        "        for inputs, labels in self.train_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            logps = self.model.forward(inputs)\n",
        "            loss = self.criterion(logps, labels)\n",
        "            loss.backward()\n",
        "            self.student_optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        traininglosses = running_loss / len(self.train_loader)\n",
        "        return traininglosses\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.test_loader:\n",
        "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                logps = self.model.forward(inputs)\n",
        "                batch_loss = criterion(logps, labels)\n",
        "                test_loss += batch_loss.item()\n",
        "                # Calculate accuracy\n",
        "                ps = torch.exp(logps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "\n",
        "        testinglosses = test_loss / len(self.test_loader)\n",
        "        testaccuracy = accuracy / len(self.test_loader)\n",
        "        return testinglosses, testaccuracy\n",
        "\n",
        "\n",
        "class student_solver():\n",
        "\n",
        "    def __init__(self, train_loader, test_loader, model, teacher_model, criterion, student_optimizer,\n",
        "                 student_lr_scheduler,\n",
        "                 epochs, model_path, model_name,temperatures=10, alphas=0.5, learning_rates=0.0001,\n",
        "                 learning_rate_decays=0.95, weight_decays=1e-5, momentums= 0.9, dropout_probabilities = (0,0)):\n",
        "\n",
        "        self.model_path = model_path\n",
        "        self.model_name = model_name\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.model = model\n",
        "        self.teacher_model = teacher_model\n",
        "        self.student_optimizer = student_optimizer\n",
        "        self.student_lr_scheduler = student_lr_scheduler\n",
        "        self.epochs = epochs\n",
        "        self.criterion = criterion\n",
        "        self.step = 0\n",
        "        self.T = temperatures\n",
        "        self.alphas = alphas\n",
        "        self.dropout_input = dropout_probabilities[0]\n",
        "        self.dropout_hidden = dropout_probabilities[1]\n",
        "        self.lr_decay = learning_rate_decays\n",
        "        self.weight_decay = weight_decays\n",
        "        self.momentum = momentums\n",
        "        self.lr = learning_rates\n",
        "\n",
        "        reproducibilitySeed()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains teacher on given hyperparameters for given number of epochs; Pass val_loader=None when not required to validate for every epoch\n",
        "        Return: List of training loss, accuracy for each update calculated only on the batch; List of validation loss, accuracy for each epoch\n",
        "        \"\"\"\n",
        "        self.model.dropout_input = self.dropout_input\n",
        "        self.model.dropout_hidden = self.dropout_hidden\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=self.lr,\n",
        "                              momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=self.lr_decay)\n",
        "        val_loss = 1\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"Start KD Training...\")\n",
        "            lr_scheduler.step()\n",
        "            start = datetime.now()\n",
        "            tr_stu_avg_loss = self.train_loop()\n",
        "            val_stu_avg_loss, testaccuracy = self.validate()\n",
        "            print('-' * 50)\n",
        "            print('Summary: Epoch {0} | Time {1}s'.format(epoch, datetime.now() - start))\n",
        "            print('Train | Loss {0:.4f}'.format(tr_stu_avg_loss))\n",
        "            print('Validate | Loss {0:.4f}'.format(val_stu_avg_loss))\n",
        "            print('Validate | Accuracy {0:.4f}'.format(testaccuracy))\n",
        "            # load the model\n",
        "            if val_loss is None or val_stu_avg_loss < val_loss:\n",
        "                val_loss = val_stu_avg_loss\n",
        "                torch.save(self.model.state_dict(), self.model_path + self.model_name)\n",
        "                best_model = epoch\n",
        "            print('best_model is on epoch:', best_model)\n",
        "\n",
        "    def train_loop(self):\n",
        "        # print_every = 1000\n",
        "        for i, data in enumerate(self.train_loader, 0):\n",
        "            X, y = data\n",
        "            X, y = X.to('cuda'), y.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            teacher_pred = None\n",
        "            if (self.alphas > 0):\n",
        "                with torch.no_grad():\n",
        "                    teacher_pred = self.teacher_model(X)\n",
        "            student_pred = self.model(X)\n",
        "            loss = self.studentLossFn(teacher_pred, student_pred, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 20)\n",
        "            optimizer.step()\n",
        "            # accuracy = float(torch.sum(torch.argmax(student_pred, dim=1) == y).item()) / y.shape[0]\n",
        "            # if i % print_every == 0:\n",
        "            #     loss, acc = self.validate()\n",
        "            #     print('train loss: %.3f, train loss: %.3f' %(loss, acc))\n",
        "        return loss\n",
        "\n",
        "    def validate(self):\n",
        "        loss, val_acc = self.getLossAccuracyOnDataset(self.model, self.test_loader, 'cuda')\n",
        "        return loss, val_acc\n",
        "\n",
        "\n",
        "    def getLossAccuracyOnDataset(self, network, dataset_loader, fast_device, criterion=None):\n",
        "        \"\"\"\n",
        "        Returns (loss, accuracy) of network on given dataset\n",
        "        \"\"\"\n",
        "        network.is_training = False\n",
        "        accuracy = 0.0\n",
        "        loss = 0.0\n",
        "        dataset_size = 0\n",
        "        for j, D in enumerate(dataset_loader, 0):\n",
        "            X, y = D\n",
        "            X = X.to(fast_device)\n",
        "            y = y.to(fast_device)\n",
        "            with torch.no_grad():\n",
        "                pred = network(X)\n",
        "                if criterion is not None:\n",
        "                    loss += criterion(pred, y) * y.shape[0]\n",
        "                accuracy += torch.sum(torch.argmax(pred, dim=1) == y).item()\n",
        "            dataset_size += y.shape[0]\n",
        "        loss, accuracy = loss / dataset_size, accuracy / dataset_size\n",
        "        network.is_training = True\n",
        "        return loss, accuracy\n",
        "\n",
        "    def studentLossFn(self, teacher_pred, student_pred, y):\n",
        "        \"\"\"\n",
        "        Loss function for student network: Loss = alpha * (distillation loss with soft-target) + (1 - alpha) * (cross-entropy loss with true label)\n",
        "        Return: loss\n",
        "        \"\"\"\n",
        "        T = self.T\n",
        "        alpha = self.alphas\n",
        "        if (alpha > 0):\n",
        "            loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1),\n",
        "                            reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (\n",
        "                           1 - alpha)\n",
        "        else:\n",
        "            loss = F.cross_entropy(student_pred, y)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def reproducibilitySeed(use_gpu=True):\n",
        "    \"\"\"\n",
        "    Ensure reproducibility of results; Seeds to 0\n",
        "    \"\"\"\n",
        "    torch_init_seed = 0\n",
        "    torch.manual_seed(torch_init_seed)\n",
        "    numpy_init_seed = 0\n",
        "    np.random.seed(numpy_init_seed)\n",
        "    if use_gpu:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n"
      ],
      "metadata": {
        "id": "ImkQ6q4QJ0f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# train_loader, test_loader = dataload('MNIST', bs=10)\n",
        "# teacher_model = Teacher_Network(in_channels=1).cuda()\n",
        "# student_model = StudentNetwork_noRelu(in_channels=1).cuda()\n",
        "\n",
        "# student_model_name = 'student.pth'\n",
        "\n",
        "# student_model.load_state_dict(torch.load(student_model_name))\n",
        "# print(\"init weight from {}\".format(student_model_name))\n",
        "# print(sum([param.nelement() * param.element_size() for param in student_model.parameters()]))\n",
        "\n",
        "# solver = student_solver(train_loader, test_loader, student_model, teacher_model, None, None,\n",
        "#                         None,\n",
        "#                         None, '', student_model_name)\n",
        "# loss, val_acc = solver.validate()\n",
        "# print('Validate | Loss {0:.4f}'.format(loss))\n",
        "# print('Validate | Accuracy {0:.4f}'.format(val_acc))"
      ],
      "metadata": {
        "id": "33d7XGOrJ_wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = StudentNetwork_noRelu(in_channels=1).cuda()\n",
        "student_model_name = 'student.pth'\n",
        "\n",
        "student_model.load_state_dict(torch.load(student_model_name))\n",
        "print(\"init weight from {}\".format(student_model_name))\n",
        "print(sum([param.nelement() * param.element_size() for param in student_model.parameters()]))\n",
        "\n",
        "test = torch.zeros(10,1,28,28).cuda()\n",
        "student_model(test)"
      ],
      "metadata": {
        "id": "11AmyevrskpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}